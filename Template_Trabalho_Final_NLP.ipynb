{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbi6PDS9MYO"
      },
      "source": [
        "***Participantes (RM - NOME):***<br>\n",
        "Vinicius - 345167<br>\n",
        "Guilherme - 346006<br>\n",
        "Fabiana - 345858<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xw6WhaNo4k3"
      },
      "source": [
        "###**Criar um classificador de chamados aplicando técnicas de PLN**\n",
        "---\n",
        "\n",
        "A **DinDinAgora** tem um canal de atendimento via chat e precisar classificar os assuntos dos atendimentos para melhorar as tratativas dos chamados dos clientes. O canal recebe textos abertos dos clientes relatando o problema e/ou dúvida e depois é direcionado para algum uma área especialista no assunto para uma melhor tratativa.​\n",
        "\n",
        "Crie um modelo classificador de assuntos aplicando técnicas de PLN, que consiga classificar através de um texto o assunto conforme disponível na base de dados [1] para treinamento e validação do modelo seu modelo.​\n",
        "\n",
        "O modelo precisar atingir um score na **métrica F1 Score superior a 75%**. Utilize o dataset [1] para treinar e testar o modelo, separe o dataset em duas amostras (75% para treinamento e 25% para teste com o randon_state igual a 42).​\n",
        "\n",
        "Fique à vontade para testar e explorar as técnicas de pré-processamento, abordagens de NLP, algoritmos e bibliotecas, mas explique e justifique as suas decisões durante o desenvolvimento.​\n",
        "\n",
        "**Composição da nota:​**\n",
        "\n",
        "**50%** - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, organização do pipeline, etc.)​\n",
        "\n",
        "**50%** - Baseado na performance (score) obtida com a amostra de teste no pipeline do modelo campeão (validar com  a Métrica F1 Score). **Separar o pipeline completo do modelo campeão conforme template.​**\n",
        "\n",
        "**[1] = ​https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv**\n",
        "\n",
        "Obs.: Para a métrica F1 Score, usar o parâmetro average = 'weighted'.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyKC9Vhkp0BK"
      },
      "source": [
        "Bom desenvolvimento!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post1-py3-none-any.whl\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n",
            "Requirement already satisfied: nltk in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: pandas in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (1.5.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from pandas) (1.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Google Colab install dependencies\n",
        "!pip install -U sklearn\n",
        "!pip install -U nltk\n",
        "!pip install -U pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘models’: File exists\n",
            "--2023-03-04 17:00:48--  https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv\n",
            "Resolving dados-ml-pln.s3.sa-east-1.amazonaws.com (dados-ml-pln.s3.sa-east-1.amazonaws.com)... 16.12.0.22\n",
            "Connecting to dados-ml-pln.s3.sa-east-1.amazonaws.com (dados-ml-pln.s3.sa-east-1.amazonaws.com)|16.12.0.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33020135 (31M) [text/csv]\n",
            "Saving to: ‘data/tickets_reclamacoes_classificados.csv’\n",
            "\n",
            "data/tickets_reclam 100%[===================>]  31.49M  34.2MB/s    in 0.9s    \n",
            "\n",
            "2023-03-04 17:00:49 (34.2 MB/s) - ‘data/tickets_reclamacoes_classificados.csv’ saved [33020135/33020135]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download dataset in colab\n",
        "!mkdir data\n",
        "!mkdir models\n",
        "!wget -O data/tickets_reclamacoes_classificados.csv https://dados-ml-pln.s3.sa-east-1.amazonaws.com/tickets_reclamacoes_classificados.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /home/vinimarcili/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /home/vinimarcili/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/vinimarcili/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem.rslp import RSLPStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('rslp')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DMBI8SQtps1n"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/tickets_reclamacoes_classificados.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s__lBzDQwrcG",
        "outputId": "9c4faa96-70b9-4255-f130-9f12e0548ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 21072 entries, 0 to 21071\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   id_reclamacao         21072 non-null  int64 \n",
            " 1   data_abertura         21072 non-null  object\n",
            " 2   categoria             21072 non-null  object\n",
            " 3   descricao_reclamacao  21072 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 658.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_reclamacao</th>\n",
              "      <th>data_abertura</th>\n",
              "      <th>categoria</th>\n",
              "      <th>descricao_reclamacao</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3229299</td>\n",
              "      <td>2019-05-01T12:00:00-05:00</td>\n",
              "      <td>Hipotecas / Empréstimos</td>\n",
              "      <td>Bom dia, meu nome é xxxx xxxx e agradeço se vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3199379</td>\n",
              "      <td>2019-04-02T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3233499</td>\n",
              "      <td>2019-05-06T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>O cartão Chase foi relatado em xx/xx/2019. No ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3180294</td>\n",
              "      <td>2019-03-14T12:00:00-05:00</td>\n",
              "      <td>Cartão de crédito / Cartão pré-pago</td>\n",
              "      <td>Em xx/xx/2018, enquanto tentava reservar um ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3224980</td>\n",
              "      <td>2019-04-27T12:00:00-05:00</td>\n",
              "      <td>Serviços de conta bancária</td>\n",
              "      <td>Meu neto me dê cheque por {$ 1600,00} Eu depos...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id_reclamacao              data_abertura  \\\n",
              "0        3229299  2019-05-01T12:00:00-05:00   \n",
              "1        3199379  2019-04-02T12:00:00-05:00   \n",
              "2        3233499  2019-05-06T12:00:00-05:00   \n",
              "3        3180294  2019-03-14T12:00:00-05:00   \n",
              "4        3224980  2019-04-27T12:00:00-05:00   \n",
              "\n",
              "                             categoria  \\\n",
              "0              Hipotecas / Empréstimos   \n",
              "1  Cartão de crédito / Cartão pré-pago   \n",
              "2  Cartão de crédito / Cartão pré-pago   \n",
              "3  Cartão de crédito / Cartão pré-pago   \n",
              "4           Serviços de conta bancária   \n",
              "\n",
              "                                descricao_reclamacao  \n",
              "0  Bom dia, meu nome é xxxx xxxx e agradeço se vo...  \n",
              "1  Atualizei meu cartão xxxx xxxx em xx/xx/2018 e...  \n",
              "2  O cartão Chase foi relatado em xx/xx/2019. No ...  \n",
              "3  Em xx/xx/2018, enquanto tentava reservar um ti...  \n",
              "4  Meu neto me dê cheque por {$ 1600,00} Eu depos...  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nze8UbKhosm9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "# Stemming\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed\n",
        "\n",
        "# Remoção de stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stopwords]\n",
        "\n",
        "# Tokenização\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    stems = stem_tokens(tokens, stemmer)\n",
        "    return stems\n",
        "\n",
        "# Criação do modelo de vetorização para pipe final, usando unigramas\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 1), min_df=1, max_df=1.0, use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
        "vectorizer.fit(df['descricao_reclamacao'])\n",
        "pickle.dump(vectorizer, open('models/vectorizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ue0nV0uVo3OZ"
      },
      "outputs": [],
      "source": [
        "# Separando em treino de teste depois da vetorização e stemização\n",
        "X = vectorizer.transform(df['descricao_reclamacao'])\n",
        "y = df['categoria']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8684364828125639"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Regressão Logistica\n",
        "model = LogisticRegression(random_state=42, max_iter=3000)\n",
        "model.fit(X_train, y_train)\n",
        "pickle.dump(model, open('models/model_logistic.pkl', 'wb'))\n",
        "y_pred = model.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "FziwgqJmw9OD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7352073180917162"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Arvore de decisão\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "pickle.dump(clf, open('models/model_tree.pkl', 'wb'))\n",
        "y_pred = clf.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "25cBRwGAw8-1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6718314148008369"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Multinominal Naive Bayes\n",
        "multi = MultinomialNB()\n",
        "multi.fit(X_train, y_train)\n",
        "pickle.dump(multi, open('models/model_multi.pkl', 'wb'))\n",
        "y_pred = multi.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.780892985183201"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Random Forest\n",
        "random = RandomForestClassifier(random_state=42)\n",
        "random.fit(X_train, y_train)\n",
        "pickle.dump(random, open('models/model_random.pkl', 'wb'))\n",
        "y_pred = random.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8579976698497366"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Linear SVC\n",
        "linear = LinearSVC()\n",
        "linear.fit(X_train, y_train)\n",
        "pickle.dump(linear, open('models/model_linear.pkl', 'wb'))\n",
        "y_pred = linear.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SiMjcWqD_m"
      },
      "source": [
        "####**Validação do professor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24EasckqG2I"
      },
      "source": [
        "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZxqHA-XCrqsD"
      },
      "outputs": [],
      "source": [
        "# Caregando dataframe\n",
        "data_final = pd.read_csv('data/tickets_reclamacoes_classificados.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stopwords e Instancia do Stemmer\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "stemmer = RSLPStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VFA-CYfawkEJ"
      },
      "outputs": [],
      "source": [
        "# Função de stemming\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "        stemmed.append(stemmer.stem(item))\n",
        "    return stemmed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BuJtvcfXo3J4"
      },
      "outputs": [],
      "source": [
        "# Remoção de stopwords\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6ULYNH6-o3Hf"
      },
      "outputs": [],
      "source": [
        "# Tokenização\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    stems = stem_tokens(tokens, stemmer)\n",
        "    return stems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3ClM-JTJo3FK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vinimarcili/anaconda3/envs/ln/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Criação do modelo de vetorizaçã para pipe final, usando unigramas\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 1), min_df=1, max_df=1.0, use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
        "vectorizer.fit(df['descricao_reclamacao'])\n",
        "pickle.dump(vectorizer, open('models/vectorizer.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separando em treino de teste depois da vetorização e stemização\n",
        "X = vectorizer.transform(df['descricao_reclamacao'])\n",
        "y = df['categoria']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8684364828125639"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Regressão Logistica\n",
        "model = LogisticRegression(random_state=42, max_iter=3000)\n",
        "model.fit(X_train, y_train)\n",
        "pickle.dump(model, open('models/model_logistic.pkl', 'wb'))\n",
        "y_pred = model.predict(X_test)\n",
        "f1_score(y_test, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Cartão de crédito / Cartão pré-pago'], dtype=object)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# teste\n",
        "model.predict(vectorizer.transform(['Ola, meu nome é João e estou com problemas com o meu cartão de crédito']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ln",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf2cfe15155a9bd961316fec96185de0feca26bdea020aab53395a8a9a358701"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
